{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = ''\n",
    "prefix = ''\n",
    "model_id = \"meta-textgeneration-llama-2-70b-f\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "instance_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opensearch-py --quiet\n",
    "!pip3 install requests_aws4auth --quiet\n",
    "!pip3 install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredFileLoader, PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3client = boto3.client('s3')\n",
    "ssm_client = boto3.client('ssm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# extend the SagemakerEndpointEmbeddings class from langchain to provide a custom embedding function\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(\n",
    "        self, texts: List[str], chunk_size: int = 5\n",
    "    ) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        st = time.time()\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "            results.extend(response)\n",
    "        time_taken = time.time() - st\n",
    "        logger.info(f\"got results for {len(texts)} in {time_taken}s, length of embeddings list is {len(results)}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# class for serializing/deserializing requests/responses to/from the embeddings model\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8') \n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "def create_sagemaker_embeddings_from_js_model(embeddings_model_endpoint_name: str, aws_region: str) -> SagemakerEndpointEmbeddingsJumpStart:\n",
    "    # all set to create the objects for the ContentHandler and \n",
    "    # SagemakerEndpointEmbeddingsJumpStart classes\n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    # note the name of the LLM Sagemaker endpoint, this is the model that we would\n",
    "    # be using for generating the embeddings\n",
    "    embeddings = SagemakerEndpointEmbeddingsJumpStart( \n",
    "        endpoint_name=embeddings_model_endpoint_name,\n",
    "        region_name=aws_region, \n",
    "        content_handler=content_handler\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = DirectoryLoader()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=args.chunk_size_for_doc_split,\n",
    "    chunk_overlap=args.chunk_overlap_for_doc_split,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Stage one: read all the docs, split them into chunks. \n",
    "logger.info('Loading documents ...')\n",
    "docs = loader.load()\n",
    "\n",
    "# add a custom metadata field, such as timestamp\n",
    "for doc in docs:\n",
    "    doc.metadata['timestamp'] = time.time()\n",
    "    doc.metadata['embeddings_model'] = args.embeddings_model_endpoint_name\n",
    "chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
    "\n",
    "embeddings = create_sagemaker_embeddings_from_js_model(args.embeddings_model_endpoint_name, args.aws_region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a OpenSearch Index\n",
    "\n",
    "Here, we use OpenSearch as a Vector Store. The first step is to create an Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = ssm_client.get_parameter(Name='AccessKey')['Parameter']['Value']\n",
    "secret_key = ssm_client.get_parameter(Name='SecretAccessKey')['Parameter']['Value']\n",
    "host = ssm_client.get_parameter(Name='OpenSearchHost')['Parameter']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = 'aoss'\n",
    "\n",
    "INDEX_NAME = 'sm_docs_' + ''.join(random.choices(string.ascii_lowercase, k=8))\n",
    "VECTOR_FIELD = 'vectors'\n",
    "\n",
    "awsauth = AWS4Auth(access_key, secret_key,\n",
    "                   aws_region, service)# session_token=credentials.token)\n",
    "\n",
    "# Create the OpenSearch client\n",
    "aoss_client = OpenSearch(\n",
    "        hosts=[{'host': host, 'port': 443}],\n",
    "        http_auth=awsauth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "##Delete the index if exists\n",
    "#response = aoss_client.indices.delete(\n",
    "#    index = INDEX_NAME\n",
    "#)\n",
    "\n",
    "#Create the index\n",
    "aoss_client.indices.create(INDEX_NAME, \n",
    "    body={\n",
    "        \"settings\":{\n",
    "            \"index.knn\": True\n",
    "        },\n",
    "        \"mappings\":{\n",
    "            \"properties\": {\n",
    "                \"vectors\": {\n",
    "                    \"type\": \"knn_vector\", \n",
    "                    \"dimension\": 1536 # dimension of the embedding vector\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs:\n",
    "    # The text data of each chunk\n",
    "    exampleContent = i.page_content\n",
    "    # Generating the embeddings for each chunk of text data\n",
    "    exampleInput = json.dumps({\"inputText\": exampleContent})\n",
    "    exampleVectors = embeddings.embed_query(exampleInput)\n",
    "\n",
    "    # setting the text data as the text variable, and generated vector to a vector variable\n",
    "    text = exampleContent\n",
    "    vectors = exampleVectors\n",
    "    \n",
    "    indexDocument = {VECTOR_FIELD: vectors,'text': text}\n",
    "   \n",
    "    response = aoss_client.index(\n",
    "        index=INDEX_NAME,\n",
    "        body=indexDocument,\n",
    "        refresh=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
